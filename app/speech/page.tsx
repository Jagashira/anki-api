"use client";

import React, { useEffect, useRef, useState } from "react";
import { Button } from "@/components/ui/button";
import { Card, CardContent, CardHeader, CardTitle } from "@/components/ui/card";
import {
  prompts,
  PromptSelector,
  PromptType,
} from "../components/speech/PromptSelector";
import ReactMarkdown from "react-markdown";
import { saveTranscriptToFirestore } from "@/app/lib/saveTranscript";
import { calculateUsage, getAudioDuration } from "@/app/lib/usage";
import TranscriptStats from "../components/speech/TranscriptStats";
import { TranscriptList } from "@/app/components/speech/TranscriptList";
import { Accordion, AccordionItem } from "@mantine/core";
import { AccordionContent, AccordionTrigger } from "@radix-ui/react-accordion";

export default function SpeechPage() {
  const [recording, setRecording] = useState(false);
  const [audioChunks, setAudioChunks] = useState<Blob[]>([]);
  const [summary, setSummary] = useState<string>("");
  const [transcript, setTranscript] = useState<string>("");
  const [audioDuration, setAudioDuration] = useState<number | null>(null);
  const [gptUsage, setGptUsage] = useState<number | null>(null);
  const mediaRecorderRef = useRef<MediaRecorder | null>(null);
  const audioContextRef = useRef<AudioContext | null>(null);
  const analyserNodeRef = useRef<AnalyserNode | null>(null);
  const mediaStreamRef = useRef<MediaStream | null>(null);
  const [promptType, setPromptType] = useState<PromptType>("simple");
  const [isCustomPrompt, setIsCustomPrompt] = useState(false);
  const [customPrompt, setCustomPrompt] = useState("");

  const startRecording = async () => {
    try {
      if (navigator.mediaDevices && navigator.mediaDevices.getUserMedia) {
        // getUserMediaãŒã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã‚‹å ´åˆã€é€šå¸¸ã®éŒ²éŸ³ã‚’è¡Œã†
        const stream = await navigator.mediaDevices.getUserMedia({
          audio: true,
        });
        const mediaRecorder = new MediaRecorder(stream);
        mediaRecorderRef.current = mediaRecorder;
        const chunks: Blob[] = [];

        mediaRecorder.ondataavailable = (e) => {
          if (e.data.size > 0) chunks.push(e.data);
        };

        mediaRecorder.onstop = async () => {
          setAudioChunks(chunks);
          const audioBlob = new Blob(chunks, { type: "audio/webm" });

          // éŸ³å£°ã®é•·ã•ã‚’å–å¾—
          const duration = await getAudioDuration(audioBlob);
          setAudioDuration(duration);

          const formData = new FormData();
          formData.append("audio", audioBlob, "audio.webm");
          formData.append("duration", duration.toString());

          const res = await fetch("/api/whisper", {
            method: "POST",
            body: formData,
          });

          const data = await res.json();

          if (data.text) {
            setTranscript(data.text);

            const summaryRes = await fetch("/api/chatgpt", {
              method: "POST",
              headers: { "Content-Type": "application/json" },
              body: JSON.stringify({
                text: data.text,
                prompt: isCustomPrompt ? customPrompt : prompts[promptType],
              }),
            });

            const summaryData = await summaryRes.json();
            setGptUsage(summaryData.tokens);
            setSummary(summaryData.summary || "è¦ç´„ã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚");

            await saveTranscriptToFirestore({
              duration,
              promptType,
              customPrompt: isCustomPrompt ? customPrompt : undefined,
              whisperText: data.text,
              chatGptSummary: summaryData.summary || "",
            });
          }
        };

        mediaRecorder.start();
        setRecording(true);
      } else {
        // getUserMediaãŒã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ãªã„å ´åˆã€AudioContextã§éŒ²éŸ³é–‹å§‹
        audioContextRef.current = new AudioContext();
        mediaStreamRef.current = await navigator.mediaDevices.getUserMedia({
          audio: true,
        });

        const analyser = audioContextRef.current.createAnalyser();
        analyserNodeRef.current = analyser;
        const mediaStreamSource =
          audioContextRef.current.createMediaStreamSource(
            mediaStreamRef.current
          );
        mediaStreamSource.connect(analyser);

        // Web Audio APIã§éŸ³å£°ã‚’å–å¾—ã—ã¦éŒ²éŸ³ï¼ˆæœªå®Ÿè£…: éŸ³å£°ãƒ‡ãƒ¼ã‚¿ä¿å­˜å‡¦ç†ï¼‰

        setRecording(true);
      }
    } catch (error) {
      console.error("éŒ²éŸ³ã«å¤±æ•—ã—ã¾ã—ãŸ", error);
      alert(
        "éŒ²éŸ³ã®é–‹å§‹ã«å¤±æ•—ã—ã¾ã—ãŸã€‚ãƒã‚¤ã‚¯ã®ã‚¢ã‚¯ã‚»ã‚¹è¨±å¯ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚"
      );
    }
  };

  const stopRecording = () => {
    mediaRecorderRef.current?.stop();
    audioContextRef.current?.close();
    setRecording(false);
  };

  useEffect(() => {
    if (!navigator.mediaDevices || !navigator.mediaDevices.getUserMedia) {
      alert("ã“ã®ãƒ–ãƒ©ã‚¦ã‚¶ã¯éŒ²éŸ³æ©Ÿèƒ½ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã¾ã›ã‚“ã€‚");
    }
  }, []);

  return (
    <div className="max-w-3xl mx-auto p-6 space-y-6">
      <Card>
        <CardHeader>
          <CardTitle className="text-2xl">ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ éŒ²éŸ³</CardTitle>
        </CardHeader>
        <CardContent className="flex gap-4">
          <Button onClick={recording ? stopRecording : startRecording}>
            {recording ? "â¹ éŒ²éŸ³åœæ­¢" : "ğŸ™ éŒ²éŸ³é–‹å§‹"}
          </Button>
        </CardContent>
      </Card>

      <Card>
        <CardHeader>
          <CardTitle>ğŸ“ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’é¸æŠ</CardTitle>
        </CardHeader>
        <CardContent>
          <p className="text-gray-800">
            éŒ²éŸ³ã—ãŸéŸ³å£°ã‚’è¦ç´„ã™ã‚‹ãŸã‚ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚
          </p>
          <PromptSelector
            promptType={promptType}
            setPromptType={setPromptType}
            customPrompt={customPrompt}
            setCustomPrompt={setCustomPrompt}
            setIsCustomPrompt={setIsCustomPrompt}
          />
        </CardContent>
      </Card>

      {audioDuration !== null && (
        <Card>
          <CardHeader>
            <CardTitle>ğŸ”Š éŸ³å£°ã®é•·ã•</CardTitle>
          </CardHeader>
          <CardContent>
            <p className="text-gray-800">
              éŒ²éŸ³ã•ã‚ŒãŸéŸ³å£°ã®é•·ã•ã¯ {audioDuration.toFixed(2)} ç§’ã§ã™ã€‚
            </p>

            <p className="text-gray-800">
              Whisperã®ã‚³ã‚¹ãƒˆã¯ç´„
              {calculateUsage(
                Number(audioDuration.toFixed(2)) * 142.044
              ).toFixed(2)}{" "}
              å†† ã§ã™ã€‚
            </p>
            <p className="text-gray-800">
              GPTã®ã‚³ã‚¹ãƒˆã¯ç´„
              {gptUsage ? (gptUsage * 0.002).toFixed(2) : "è¨ˆç®—ä¸­"} å††ã§ã™ã€‚
            </p>
          </CardContent>
        </Card>
      )}

      {summary && (
        <Card>
          <CardHeader>
            <CardTitle>ğŸ“ è¦ç´„çµæœ</CardTitle>
          </CardHeader>
          <CardContent>
            <div className="prose prose-neutral max-w-none">
              <ReactMarkdown>{summary}</ReactMarkdown>
            </div>
          </CardContent>
        </Card>
      )}

      {transcript && (
        //@ts-ignore
        <Accordion type="single" collapsible className="w-full">
          <AccordionItem value="transcript">
            <AccordionTrigger>ğŸ“„ æ–‡å­—èµ·ã“ã—ã‚’è¦‹ã‚‹</AccordionTrigger>
            <AccordionContent>
              <div className="bg-gray-50 p-4 rounded-md text-sm whitespace-pre-wrap max-h-96 overflow-y-auto">
                {transcript}
              </div>
            </AccordionContent>
          </AccordionItem>
        </Accordion>
      )}
      <TranscriptStats />
      <div className="p-6">
        <h1 className="text-xl font-bold mb-4">Transcriptä¸€è¦§</h1>
        <TranscriptList />
      </div>
    </div>
  );
}
